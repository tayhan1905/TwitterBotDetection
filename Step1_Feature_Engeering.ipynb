{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Tweets Sampling from the tweet dataset\n",
    "\n",
    "## 1.1 Querying from MongoDB\n",
    "Due to the limitations of the computational power of our devices, we have decided to split the 8 tweet datasets up amongst ourselves (our group). The script below depicts the sequential steps to take in order to derive the final graph and user datasets for each of the tweet files.\n",
    "\n",
    "Due to the sheer size of the datasets, we had to utilise MongoDB to assist in the querying and sampling of the data that was required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your db and collection name accordingly\n",
    "hostname = 'localhost'\n",
    "port = 27017  \n",
    "client = MongoClient(hostname, port)\n",
    "db = client['BT4222'] #Change your db accordingly\n",
    "tweets = db['Tweets_0'] #change your tweet data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for all retweets that are in English and extract author_ids\n",
    "pipeline = [\n",
    "    {'$match': {'lang': 'en', 'text': {'$regex':'^RT'}}},\n",
    "    {'$project': {'_id': 0, 'author_id': 1}}\n",
    "]\n",
    "user_cursor = tweets.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique author_id: 0\n"
     ]
    }
   ],
   "source": [
    "#Identifying the number of unique author_ids\n",
    "unique_ids = set()\n",
    "for record in user_cursor:\n",
    "    unique_ids.add(record['author_id'])\n",
    "\n",
    "unique_count = len(unique_ids)\n",
    "print(\"Number of unique author_id:\", unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load the labelled dataset `label.csv` to find out which users are bots and humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hengboonlong/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Twibot-22/label.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the label dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/hengboonlong/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Twibot-22/label.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hengboonlong/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Twibot-22/label.csv'"
     ]
    }
   ],
   "source": [
    "# load the label dataset\n",
    "labels = pd.read_csv(\"label.csv\")\n",
    "labels[\"id\"] = labels[\"id\"].str.replace(\"u\",\"\")\n",
    "labels[\"id\"] = labels[\"id\"].astype(\"int64\")\n",
    "\n",
    "# convert label to human: 0, bot: 1\n",
    "labels[\"label\"] = labels[\"label\"].str.replace(\"human\",\"0\")\n",
    "labels[\"label\"] = labels[\"label\"].str.replace(\"bot\",\"1\")\n",
    "labels[\"label\"] = labels[\"label\"].astype(\"int\")\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Engage in Disproportionate Stratified Sampling to get equal number of bots and humans\n",
    "Clearly, the bot data present in the dataset is very limited and this has led to a huge imbalance within the data set. To answer the severe underrepresentation of bots within the dataset, we have decided to utilise a disproportionate stratified sampling to obtain an equal number of bots and humans.\n",
    "\n",
    "### Limitations \n",
    "- Risk of underrepresentation of majority class (Human Dataset) / eliminating too much information from the Human Dataset\n",
    "\n",
    "### Mitigating Measures\n",
    "- Due to the comprehensiveness and size of our dataset, we were still able to retain 50,000 human ids and thus, we believe that the 50,000 human ids can adequately represent the human population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bots: 11427\n",
      "Number of humans: 173822\n"
     ]
    }
   ],
   "source": [
    "# Identifying bot and human ids\n",
    "bot_ids = []\n",
    "human_ids = []\n",
    "\n",
    "for id in unique_ids:\n",
    "    try:\n",
    "        if (labels[labels[\"id\"]==id][\"label\"].values[0] == 0):\n",
    "            human_ids.append(id)\n",
    "        elif (labels[labels[\"id\"]==id][\"label\"].values[0] == 1):\n",
    "            bot_ids.append(id)\n",
    "            \n",
    "    # if id doesnt exist in label dataset\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print(f\"Number of bots: {len(bot_ids)}\") \n",
    "print(f\"Number of humans: {len(human_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples per class: 9142\n",
      "Number of sampled bots: 9142\n",
      "Number of sampled humans: 9142\n",
      "Number of sampled ids: 18284\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from random import sample\n",
    "\n",
    "# Sample 80% of the minority class (bots)\n",
    "n_samples_per_class = ceil(0.8 * len(bot_ids))\n",
    "print(f\"Number of samples per class: {n_samples_per_class}\")\n",
    "\n",
    "sampled_human_ids = sample(human_ids, n_samples_per_class)\n",
    "sampled_bot_ids = sample(bot_ids, n_samples_per_class)\n",
    "sampled_ids = sampled_human_ids + sampled_bot_ids\n",
    "\n",
    "print(f\"Number of sampled bots: {len(sampled_bot_ids)}\") \n",
    "print(f\"Number of sampled humans: {len(sampled_human_ids)}\")\n",
    "print(f\"Number of sampled ids: {len(sampled_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Extract all tweets from MongoDB where the authors or tweet mentions are in sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {'$match': {'$or': [{'author_id': {'$in': sampled_ids}}, {'entities.user_mention.id': {'$in': sampled_ids}}]}},\n",
    "    {'$project': {\n",
    "        '_id': 0, \n",
    "        'author_id': 1, \n",
    "        'conversation_id': 1, \n",
    "        'created_at': 1, \n",
    "        'id': 1, \n",
    "        'text': 1, \n",
    "        'user_mentions_count': {\n",
    "            '$cond': {\n",
    "                'if': {'$isArray': \"$entities.user_mentions\"},\n",
    "                'then': {'$size': \"$entities.user_mentions\"},\n",
    "                'else': 0\n",
    "            }\n",
    "        },\n",
    "        'hashtags_count': {\n",
    "            '$cond': {\n",
    "                'if': {'$isArray': \"$entities.hashtags\"},\n",
    "                'then': {'$size': \"$entities.hashtags\"},\n",
    "                'else': 0\n",
    "            }\n",
    "        },\n",
    "        'symbols_count': {\n",
    "            '$cond': {\n",
    "                'if': {'$isArray': \"$entities.symbols\"},\n",
    "                'then': {'$size': \"$entities.symbols\"},\n",
    "                'else': 0\n",
    "            }\n",
    "        },\n",
    "        'urls_count': {\n",
    "            '$cond': {\n",
    "                'if': {'$isArray': \"$entities.urls\"},\n",
    "                'then': {'$size': \"$entities.urls\"},\n",
    "                'else': 0\n",
    "            }\n",
    "        },\n",
    "        'mentioned_user_id': {'$arrayElemAt': [\"$entities.user_mentions.id\", 0]},\n",
    "        'in_reply_to_user_id': 1, \n",
    "        'retweet_count': '$public_metrics.retweet_count', \n",
    "        'reply_count': '$public_metrics.reply_count', \n",
    "        'like_count': '$public_metrics.like_count', \n",
    "        'quote_count': '$public_metrics.quote_count'\n",
    "    }}\n",
    "]\n",
    "\n",
    "tweet_cursor = tweets.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_mentions_count</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>symbols_count</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>mentioned_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3971456121</td>\n",
       "      <td>1496841052032012288</td>\n",
       "      <td>2022-02-24 13:34:51+00:00</td>\n",
       "      <td>t1496841052032012293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>💥Today at 5PM CET - the first ERA Journal Club...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3971456121</td>\n",
       "      <td>1496753536889204736</td>\n",
       "      <td>2022-02-24 07:47:06+00:00</td>\n",
       "      <td>t1496753536889204736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>🆕European Renal Best Practice endorsement of\\n...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.037351e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3971456121</td>\n",
       "      <td>1496407866793775104</td>\n",
       "      <td>2022-02-23 08:53:32+00:00</td>\n",
       "      <td>t1496407866793775111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>📣NEW Editorial:\\nWhat the seminal experience o...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.187608e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3971456121</td>\n",
       "      <td>1496142253743644672</td>\n",
       "      <td>2022-02-22 15:18:04+00:00</td>\n",
       "      <td>t1496142253743644684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>🚨Only a few days left to check out our Article...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.836129e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3971456121</td>\n",
       "      <td>1495740814122754048</td>\n",
       "      <td>2022-02-21 12:42:54+00:00</td>\n",
       "      <td>t1495740814122754048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Stones__: Another #AnimalHouse pearl court...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.313196e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id      conversation_id                 created_at  \\\n",
       "0  3971456121  1496841052032012288  2022-02-24 13:34:51+00:00   \n",
       "1  3971456121  1496753536889204736  2022-02-24 07:47:06+00:00   \n",
       "2  3971456121  1496407866793775104  2022-02-23 08:53:32+00:00   \n",
       "3  3971456121  1496142253743644672  2022-02-22 15:18:04+00:00   \n",
       "4  3971456121  1495740814122754048  2022-02-21 12:42:54+00:00   \n",
       "\n",
       "                     id  in_reply_to_user_id  \\\n",
       "0  t1496841052032012293                  NaN   \n",
       "1  t1496753536889204736                  NaN   \n",
       "2  t1496407866793775111                  NaN   \n",
       "3  t1496142253743644684                  NaN   \n",
       "4  t1495740814122754048                  NaN   \n",
       "\n",
       "                                                text  user_mentions_count  \\\n",
       "0  💥Today at 5PM CET - the first ERA Journal Club...                    0   \n",
       "1  🆕European Renal Best Practice endorsement of\\n...                    4   \n",
       "2  📣NEW Editorial:\\nWhat the seminal experience o...                    2   \n",
       "3  🚨Only a few days left to check out our Article...                    1   \n",
       "4  RT @Stones__: Another #AnimalHouse pearl court...                    3   \n",
       "\n",
       "   hashtags_count  symbols_count  urls_count  retweet_count  reply_count  \\\n",
       "0               0              0           1              1          NaN   \n",
       "1               0              0           1             11          NaN   \n",
       "2               0              0           1              0          NaN   \n",
       "3               0              0           1             11          NaN   \n",
       "4               3              0           0              4          NaN   \n",
       "\n",
       "   like_count  quote_count  mentioned_user_id  \n",
       "0           0          NaN                NaN  \n",
       "1           7          NaN       2.037351e+07  \n",
       "2           1          NaN       8.187608e+07  \n",
       "3          21          NaN       4.836129e+09  \n",
       "4           0          NaN       1.313196e+09  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame(list(tweet_cursor))\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new column `relationship` to classify tweet as a `retweet` \\ `reply` \\ `post`\n",
    "- If a tweet starts with \"RT\", it is a retweet\n",
    "- If a tweet has a \"in_reply_to_user_id\" that is non-null, it is a reply to the said user_id\n",
    "- Else, it will be a normal post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"relationship\" -> retweet/reply/post\n",
    "def add_relationship(df):\n",
    "    if (df[\"text\"][:2] == \"RT\"):\n",
    "        return \"Retweet\"\n",
    "    elif (pd.notna(df[\"in_reply_to_user_id\"])):\n",
    "        return \"Reply\"\n",
    "    else:\n",
    "        return \"Post\"\n",
    "\n",
    "tweets_df[\"relationship\"] = tweets_df.apply(lambda x : add_relationship(x), axis=1)\n",
    "\n",
    "# Add a new column to define the target_user_id\n",
    "def target_user(df):\n",
    "    if (df[\"relationship\"] == \"Retweet\"):\n",
    "        return df[\"mentioned_user_id\"]\n",
    "    elif (df[\"relationship\"] == \"Reply\"):\n",
    "        return df[\"in_reply_to_user_id\"]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "tweets_df[\"target_user_id\"] = tweets_df.apply(lambda x : target_user(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the \"in_reply_to_user_id\" and \"mentioned_user_id\"\n",
    "tweets_df = tweets_df.drop([\"in_reply_to_user_id\", \"mentioned_user_id\"], axis=1)\n",
    "tweets_df = tweets_df.rename({\"author_id\": \"source_user_id\", \"id\": \"tweet_id\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all rows with null `target_user_id` as it does not capture any relationship between two users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique source and target user ids: 219395\n"
     ]
    }
   ],
   "source": [
    "# Check what is the total unique source_user_id and target_user_id in the dataset\n",
    "unique_ids = set(np.concatenate((tweets_df[\"source_user_id\"],tweets_df[tweets_df[\"target_user_id\"].notnull()][\"target_user_id\"])))\n",
    "print(f\"Number of unique source and target user ids: {len(unique_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of all the source and target user ids without a label\n",
    "This means that the source and target user id has no user metadata and thus, will be removed from our overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bots: 11699\n",
      "Number of humans: 83697\n",
      "Number of ids with no label: 123999\n"
     ]
    }
   ],
   "source": [
    "bot_ids = []\n",
    "human_ids = []\n",
    "ids_no_label = []\n",
    "\n",
    "for id in unique_ids:\n",
    "    try:\n",
    "        if (labels[labels[\"id\"]==id][\"label\"].values[0] == 0):\n",
    "            human_ids.append(id)\n",
    "        elif (labels[labels[\"id\"]==id][\"label\"].values[0] == 1):\n",
    "            bot_ids.append(id)\n",
    "            \n",
    "    # if id doesnt exist in label dataset\n",
    "    except:\n",
    "        ids_no_label.append(id)\n",
    "\n",
    "print(f\"Number of bots: {len(bot_ids)}\") \n",
    "print(f\"Number of humans: {len(human_ids)}\")\n",
    "print(f\"Number of ids with no label: {len(ids_no_label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Utilise Disproportionate Stratified Sampling to ensure that final dataset includes equal number of bots and humans in both `source_user_id` and `target_user_id`\n",
    "\n",
    "Although disproportionate sacrifices some precision in the estimate of the majority class, in the context of our project, is great for identifying differences between underrepresented groups (bots) and the majority class (humans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that we have same number of human and bots captured in the tweet dataset\n",
    "num_bots = len(bot_ids)\n",
    "num_humans = len(human_ids)\n",
    "if (num_bots < num_humans):\n",
    "    sampled_human_ids = sample(human_ids, ceil(1.3*num_bots))\n",
    "    final_ids = bot_ids + sampled_human_ids\n",
    "else:\n",
    "    sampled_bot_ids = sample(bot_ids, ceil(1.3*num_humans))\n",
    "    final_ids = human_ids + sampled_bot_ids\n",
    "\n",
    "# Drop any rows where the source_user_id and target_user_id not in final_ids\n",
    "tweets_df = tweets_df.drop(tweets_df[(tweets_df[\"target_user_id\"].notnull()) & ((~tweets_df[\"target_user_id\"].isin(final_ids)) | (~tweets_df[\"source_user_id\"].isin(final_ids)))].index, axis=0)\n",
    "tweets_df = tweets_df.drop(tweets_df[(tweets_df[\"target_user_id\"].isnull()) & (~tweets_df[\"source_user_id\"].isin(final_ids))].index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Counts of valid Human and Bot IDs within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique source and target user ids: 21457\n",
      "Number of bots: 10763\n",
      "Number of humans: 10694\n",
      "Number of ids with no label: 0\n"
     ]
    }
   ],
   "source": [
    "unique_ids = set(np.concatenate((tweets_df[\"source_user_id\"],tweets_df[tweets_df[\"target_user_id\"].notnull()][\"target_user_id\"])))\n",
    "print(f\"Number of unique source and target user ids: {len(unique_ids)}\")\n",
    "\n",
    "bot_id = 0\n",
    "human_id = 0\n",
    "id_no_label = 0\n",
    "\n",
    "for id in unique_ids:\n",
    "    try:\n",
    "        if (labels[labels[\"id\"]==id][\"label\"].values[0] == 0):\n",
    "            human_id += 1\n",
    "        elif (labels[labels[\"id\"]==id][\"label\"].values[0] == 1):\n",
    "            bot_id += 1\n",
    "            \n",
    "    # if id doesnt exist in label dataset\n",
    "    except:\n",
    "        id_no_label += 1\n",
    "        \n",
    "print(f\"Number of bots: {bot_id}\") \n",
    "print(f\"Number of humans: {human_id}\")\n",
    "print(f\"Number of ids with no label: {id_no_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Querying relevant user metadata based on the final set of sample ids (Section 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Accessing MongoDB for valid user's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = db['users']\n",
    "\n",
    "# Define your query (empty to fetch all documents)\n",
    "query = {}\n",
    "\n",
    "# Specify the fields you want to include (1) or exclude (0)\n",
    "projection = {\n",
    "    'id' : 1,\n",
    "    'created_at' : 1,\n",
    "    'description' : 1,\n",
    "    'location' : 1,\n",
    "    'name' : 1,\n",
    "    'profile_image_url' : 1, \n",
    "    'public_metrics': 1,\n",
    "    'url' : 1,\n",
    "    'username' : 1,\n",
    "    'verified' : 1,\n",
    "    'entities' : 1\n",
    "}\n",
    "\n",
    "# Execute the query\n",
    "documents = users.find(query, projection)\n",
    "\n",
    "# Convert the query result to a pandas DataFrame\n",
    "df = pd.DataFrame(list(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising nested jsons within the dataframe\n",
    "df = pd.concat([df, json_normalize(df[\"entities\"])], axis = 1)\n",
    "df = pd.concat([df, json_normalize(df[\"public_metrics\"])], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing\n",
    "- ensuring the correct data type for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verified\n",
    "df['verified'] = df['verified'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "#urls\n",
    "df[\"url.urls\"] = df[\"url.urls\"].fillna(0) \n",
    "df[\"url.urls\"] = df[\"url.urls\"].apply(lambda x : len(x) if x else 0)\n",
    "\n",
    "#number of urls in their descriptions\n",
    "df[\"description.urls\"] = df[\"description.urls\"].fillna(0) \n",
    "df[\"description.urls\"] = df[\"description.urls\"].apply(lambda x : len(x) if x else 0)\n",
    "\n",
    "#number of mentions\n",
    "df[\"description.mentions\"] = df[\"description.mentions\"].fillna(0) \n",
    "df[\"description.mentions\"] = df[\"description.mentions\"].apply(lambda x : len(x) if x else 0)\n",
    "\n",
    "#number of hashtags\n",
    "df[\"description.hashtags\"] = df[\"description.hashtags\"].fillna(0) \n",
    "df[\"description.hashtags\"] = df[\"description.hashtags\"].apply(lambda x : len(x) if x else 0)\n",
    "\n",
    "#number of cashtags\n",
    "df[\"description.cashtags\"] = df[\"description.cashtags\"].fillna(0) \n",
    "df[\"description.cashtags\"] = df[\"description.cashtags\"].apply(lambda x : len(x) if x else 0)\n",
    "\n",
    "df = df.drop(['entities', 'public_metrics'], axis = 1)\n",
    "\n",
    "#align the user information with source_user_id to serve as the key for join\n",
    "\n",
    "df[\"source_user_id\"] = df[\"id\"].apply(lambda x : int(x.replace(\"u\", \"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mixed_timestamps(ts):\n",
    "    try:\n",
    "        # Attempt to convert using the ISO format\n",
    "        return pd.to_datetime(ts, exact=False, errors='raise')\n",
    "    except ValueError:\n",
    "        # Check if ts is not convertable to float (e.g., empty string, '[]')\n",
    "        try:\n",
    "            # This will fail if ts is not a valid string representation of a float\n",
    "            float_ts = float(ts)\n",
    "        except ValueError:\n",
    "            # If conversion fails, return None or handle as needed\n",
    "            return None\n",
    "        else:\n",
    "            # If conversion is successful, proceed to convert to datetime\n",
    "            return pd.to_datetime(float_ts / 1e9, unit='s', utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Extraction I (User Profile Features)\n",
    "Refer to the report for the mathematical representations / formulas of each of the Features. The intuition behind the creation of each feature is also detailed in the report with references from relevant scholarship.\n",
    "\n",
    "1) `Username length`\n",
    "2) `Name length`\n",
    "3) `Description length`\n",
    "4) `Number of digits in username`\n",
    "5) `Entropy of username`\n",
    "6) `Entropy of description`\n",
    "7) `Name and username similarity`\n",
    "8) `Ratio of length of username to length of name`\n",
    "10) `Reputation`\n",
    "11) `Age of the account` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 2 & 3. `Username, name and description length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"username_length\"] = df[\"username\"].apply(len)\n",
    "df[\"name_length\"] = df[\"name\"].apply(len)\n",
    "df[\"description_length\"] = df[\"description\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `Number of digits in username`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numDigits_username'] = df['username'].apply(lambda x: sum(c.isdigit() for c in x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 & 6. `Entropy of username and description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate entropy of a string\n",
    "def entropy(s):\n",
    "    probabilities = [n_x/len(s) for x, n_x in Counter(s).items()]\n",
    "    e = -sum([p * math.log(p) / math.log(2.0) for p in probabilities])\n",
    "    return e\n",
    "\n",
    "df['username_entropy'] = df['username'].apply(entropy)\n",
    "df['description_entropy'] = df['description'].apply(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `Name and username similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_similarity(str1, str2):\n",
    "    matches = sum(1 for a, b in zip(str1, str2) if a == b)\n",
    "    total_length = len(str1) + len(str2)\n",
    "    similarity_score = (2 * matches) / total_length if total_length > 0 else 0\n",
    "    return similarity_score\n",
    "\n",
    "df['names_similarity'] = df.apply(lambda x: name_similarity(x['name'], x['username']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. `Ratio of name length to username length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['names_ratio'] = df.apply(lambda x: len(x['username']) / len(x['name']) if len(x['name']) > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. `Reputation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reputation\"] = df['following_count'] / (df['followers_count'] + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. `Age of the account` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current date - user creation date / total number of tweets\n",
    "#convert created_at to timestamp\n",
    "df['created_at_timestamp'] = df['created_at'].apply(convert_mixed_timestamps)\n",
    "current_date = pd.Timestamp.now().tz_localize('UTC') \n",
    "df['age_of_account'] = (current_date - df[\"created_at_timestamp\"]).dt.days / df[\"tweet_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Feature Extraction II (Temporal Features)\n",
    "1) `Retweet Ratio`\n",
    "2) `URL Ratio`\n",
    "3) `Max number of URLs in tweets` \n",
    "4) `Tweet Time Standard Deviation`\n",
    "5) `Mention Ratio` \n",
    "6) `Max number of mentions in tweets`\n",
    "7) `Hashtag Ratio`\n",
    "8) `Max Number of Hashtags in tweet` \n",
    "10) `Average Length of Tweets by user`\n",
    "11) `Average number of tweets containing URLs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting column `source_user_id` to Integer whilst dropping any invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic rows at indices: []\n"
     ]
    }
   ],
   "source": [
    "# Identification of problematic rows that are present\n",
    "\n",
    "problematic_rows = []\n",
    "for index, row in g.iterrows():\n",
    "    try:\n",
    "        _ = int(row['source_user_id']) \n",
    "    except ValueError:\n",
    "        problematic_rows.append(index) \n",
    "\n",
    "# Display problematic rows\n",
    "print(\"Problematic rows at indices:\", problematic_rows)\n",
    "graph = g.drop(problematic_rows).reset_index(drop = True)\n",
    "\n",
    "g[\"source_user_id\"] = g[\"source_user_id\"].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting `created_at_timestamp` to `datetime` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to timestamp\n",
    "g['created_at_timestamp'] = g['created_at'].apply(convert_mixed_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NA values for `retweet_count`, `reply_count`, `like_count`, `quote_count` with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling NAN values\n",
    "g['retweet_count'] = g['retweet_count'].fillna(0)\n",
    "g['reply_count'] = g['reply_count'].fillna(0)\n",
    "g['like_count'] = g['like_count'].fillna(0)\n",
    "g['quote_count'] = g['quote_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for only valid users from g (valid `source_user_id` and `target_user_id`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = set(np.concatenate((g[\"source_user_id\"],g[g[\"target_user_id\"].notnull()][\"target_user_id\"])))\n",
    "user = df[df[\"source_user_id\"].isin(user_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join user dataframe with labels dataframe so that each user can be tagged to their label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21457 entries, 0 to 21456\n",
      "Data columns (total 33 columns):\n",
      " #   Column                Non-Null Count  Dtype              \n",
      "---  ------                --------------  -----              \n",
      " 0   _id                   21457 non-null  object             \n",
      " 1   created_at            21457 non-null  object             \n",
      " 2   description           21457 non-null  object             \n",
      " 3   id_x                  21457 non-null  object             \n",
      " 4   location              16243 non-null  object             \n",
      " 5   name                  21457 non-null  object             \n",
      " 6   profile_image_url     21457 non-null  object             \n",
      " 7   url                   21457 non-null  object             \n",
      " 8   username              21457 non-null  object             \n",
      " 9   verified              21457 non-null  bool               \n",
      " 10  url.urls              21457 non-null  int64              \n",
      " 11  description.urls      21457 non-null  int64              \n",
      " 12  description.mentions  21457 non-null  int64              \n",
      " 13  description.hashtags  21457 non-null  int64              \n",
      " 14  description.cashtags  21457 non-null  int64              \n",
      " 15  followers_count       21457 non-null  int64              \n",
      " 16  following_count       21457 non-null  int64              \n",
      " 17  tweet_count           21457 non-null  int64              \n",
      " 18  listed_count          21457 non-null  int64              \n",
      " 19  source_user_id        21457 non-null  int64              \n",
      " 20  username_length       21457 non-null  int64              \n",
      " 21  name_length           21457 non-null  int64              \n",
      " 22  description_length    21457 non-null  int64              \n",
      " 23  numDigits_username    21457 non-null  int64              \n",
      " 24  username_entropy      21457 non-null  float64            \n",
      " 25  description_entropy   21457 non-null  float64            \n",
      " 26  names_similarity      21457 non-null  float64            \n",
      " 27  names_ratio           21457 non-null  float64            \n",
      " 28  reputation            21457 non-null  float64            \n",
      " 29  created_at_timestamp  21457 non-null  datetime64[ns, UTC]\n",
      " 30  age_of_account        21457 non-null  float64            \n",
      " 31  id_y                  21457 non-null  int64              \n",
      " 32  label                 21457 non-null  int64              \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(6), int64(16), object(9)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "user = user.merge(labels, left_on=\"source_user_id\", right_on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = user.drop(\"_id\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Retweet Ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10511"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retweet_ratio(series):\n",
    "    if 'Retweet' in series.value_counts():\n",
    "        return (series.value_counts()['Retweet'] + 1) / (len(series) + 3)\n",
    "    else:\n",
    "        return 1/(len(series) + 3)\n",
    "\n",
    "retweet_ratios = pd.DataFrame(g.groupby('source_user_id')['relationship'].agg(retweet_ratio))\n",
    "\n",
    "retweet_ratios['source_user_id'] = retweet_ratios.index\n",
    "\n",
    "retweet_ratios.reset_index(drop=True, inplace=True)\n",
    "\n",
    "retweet_ratios.columns = ['retweet_ratio', 'source_user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Retweet Ratios into user dataframe\n",
    "user = pd.merge(user, retweet_ratios, on='source_user_id', how='left')\n",
    "\n",
    "# fill the null values with the mean retweet ratio\n",
    "user['retweet_ratio'] = user.groupby('label')['retweet_ratio'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"retweet_ratio\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `URL Ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL Ratio = number of URLs posted per user / total number of tweets of user \n",
    "\n",
    "#Aggregate URLs\n",
    "url_count = pd.DataFrame(g.groupby('source_user_id')['urls_count'].sum())\n",
    "url_count['source_user_id'] = url_count.index\n",
    "url_count.reset_index(drop=True, inplace=True)\n",
    "url_count.columns = ['url_count_tweets', 'source_user_id']\n",
    "user = pd.merge(user, url_count, on='source_user_id', how='left')\n",
    "\n",
    "#calculating the url_ratio\n",
    "user['url_ratio'] = (user[\"url.urls\"] + user[\"url_count_tweets\"]) / user[\"tweet_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean url ratio \n",
    "user['url_ratio'] = user.groupby('label')['url_ratio'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"url_ratio\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `Max Number of URLs per tweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate URLs\n",
    "url_max = pd.DataFrame(g.groupby('source_user_id')['urls_count'].max())\n",
    "url_max['source_user_id'] = url_max.index\n",
    "url_max.reset_index(drop=True, inplace=True)\n",
    "url_max.columns = ['url_max_tweets', 'source_user_id']\n",
    "user = pd.merge(user, url_max, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean max number of URLs per tweet \n",
    "user['url_max_tweets'] = user.groupby('label')['url_max_tweets'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"url_max_tweets\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `Tweet Time Standard Deviation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_time_sd(series):\n",
    "    if len(series) < 2:\n",
    "        return np.nan\n",
    "    time_diffs = series.diff().dt.total_seconds()[1:]\n",
    "\n",
    "    mean_time_diff = time_diffs.mean()\n",
    "\n",
    "    sd = (time_diffs - mean_time_diff) ** 2\n",
    "\n",
    "    mean = np.sqrt(sd.mean()) / 86400\n",
    "\n",
    "    return mean \n",
    "\n",
    "tweet_time_SD = pd.DataFrame(g.groupby('source_user_id')['created_at_timestamp'].agg(tweet_time_sd))\n",
    "tweet_time_SD['source_user_id'] = tweet_time_SD.index\n",
    "tweet_time_SD.reset_index(drop=True, inplace=True)\n",
    "tweet_time_SD.columns = ['time_interval_sd_day', 'source_user_id']\n",
    "\n",
    "user = pd.merge(user, tweet_time_SD, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean tweet time std \n",
    "user['time_interval_sd_day'] = user.groupby('label')['time_interval_sd_day'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"time_interval_sd_day\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `Mention Ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mention Ratio = number of Mentions / total number of tweets of user u\n",
    "\n",
    "#Aggregate Mentions\n",
    "mention_count = pd.DataFrame(g.groupby('source_user_id')['user_mentions_count'].sum())\n",
    "mention_count['source_user_id'] = mention_count.index\n",
    "mention_count.reset_index(drop=True, inplace=True)\n",
    "mention_count.columns = ['mention_count_tweets', 'source_user_id']\n",
    "\n",
    "#Addition into user\n",
    "user = pd.merge(user, mention_count, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean mention ratio \n",
    "user['mention_count_tweets'] = user.groupby('label')['mention_count_tweets'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"mention_count_tweets\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. `Max Number of Mention per tweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate Mentions\n",
    "mention_max = pd.DataFrame(g.groupby('source_user_id')['user_mentions_count'].max())\n",
    "mention_max['source_user_id'] = mention_max.index\n",
    "mention_max.reset_index(drop=True, inplace=True)\n",
    "mention_max.columns = ['mention_max_tweets', 'source_user_id']\n",
    "user = pd.merge(user, mention_max, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean max number of mention per tweet \n",
    "user['mention_max_tweets'] = user.groupby('label')['mention_max_tweets'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"mention_max_tweets\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `Hashtags Ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags Ratio = number of Mentions / total number of tweets of user u\n",
    "\n",
    "#Aggregate Mentions\n",
    "hashtag_count = pd.DataFrame(g.groupby('source_user_id')['hashtags_count'].sum())\n",
    "hashtag_count['source_user_id'] = hashtag_count.index\n",
    "hashtag_count.reset_index(drop=True, inplace=True)\n",
    "hashtag_count.columns = ['hashtag_count_tweets', 'source_user_id']\n",
    "\n",
    "#Addition into user\n",
    "user = pd.merge(user, hashtag_count, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean hashtag ratio \n",
    "user['hashtag_count_tweets'] = user.groupby('label')['hashtag_count_tweets'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"hashtag_count_tweets\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. `Max Number of Hashtags per tweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate Hashtags\n",
    "hashtags_max = pd.DataFrame(g.groupby('source_user_id')['hashtags_count'].max())\n",
    "hashtags_max['source_user_id'] = hashtags_max.index\n",
    "hashtags_max.reset_index(drop=True, inplace=True)\n",
    "hashtags_max.columns = ['hashtags_max_tweets', 'source_user_id']\n",
    "user = pd.merge(user, hashtags_max, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the null values with the mean max number of hashtag per tweet \n",
    "user['hashtags_max_tweets'] = user.groupby('label')['hashtags_max_tweets'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user[\"hashtags_max_tweets\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 & 10. `Average tweet length`\n",
    "`Number of tweets`\n",
    "`Number of tweets with URLs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total tweet count\n",
    "tweet_counts = g.groupby('source_user_id').size().reset_index(name='tweet_count')\n",
    "\n",
    "# average length\n",
    "g['tweet_length'] = g['text'].str.len()\n",
    "avg_tweet_length = g.groupby('source_user_id')['tweet_length'].mean().reset_index(name='avg_tweet_length')\n",
    "\n",
    "def contains_anything(lst):\n",
    "    if isinstance(lst, list):\n",
    "        return len(lst) > 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "tweets_with_url_counts = g[g['urls_count']>0].groupby('source_user_id').size().reset_index(name='url_tweet_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining user, tweet_counts, avg_tweet_length, tweet_with_url_counts\n",
    "user = pd.merge(user, tweet_counts, on='source_user_id', how='left')\n",
    "user = pd.merge(user, avg_tweet_length, on='source_user_id', how='left')\n",
    "user = pd.merge(user, tweets_with_url_counts, on='source_user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.rename(columns={'tweet_count_y': 'tweet_count'}, inplace=True)\n",
    "user = user.drop(\"tweet_count\",axis=1)\n",
    "user = user.rename({'tweet_count_x': 'tweet_count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                 0\n",
       "description                0\n",
       "id_x                       0\n",
       "location                5214\n",
       "name                       0\n",
       "profile_image_url          0\n",
       "url                        0\n",
       "username                   0\n",
       "verified                   0\n",
       "url.urls                   0\n",
       "description.urls           0\n",
       "description.mentions       0\n",
       "description.hashtags       0\n",
       "description.cashtags       0\n",
       "followers_count            0\n",
       "following_count            0\n",
       "tweet_count                0\n",
       "listed_count               0\n",
       "source_user_id             0\n",
       "username_length            0\n",
       "name_length                0\n",
       "description_length         0\n",
       "numDigits_username         0\n",
       "username_entropy           0\n",
       "description_entropy        0\n",
       "names_similarity           0\n",
       "names_ratio                0\n",
       "reputation                 0\n",
       "created_at_timestamp       0\n",
       "age_of_account             0\n",
       "id_y                       0\n",
       "label                      0\n",
       "retweet_ratio              0\n",
       "url_count_tweets           0\n",
       "url_ratio                  0\n",
       "url_max_tweets             0\n",
       "time_interval_sd_day       0\n",
       "mention_count_tweets       0\n",
       "mention_max_tweets         0\n",
       "hashtag_count_tweets       0\n",
       "hashtags_max_tweets        0\n",
       "avg_tweet_length           0\n",
       "url_tweet_count            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"url_count_tweets\",\"avg_tweet_length\",\"url_tweet_count\"]\n",
    "\n",
    "# fill the null values with the mean value \n",
    "for col in cols:\n",
    "    user[col] = user.groupby('label')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# make sure no null values left\n",
    "user.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Feature Learning (Tweet Embedding) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Tokenisation\n",
    "- Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hengboonlong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hengboonlong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#preprocess tweets\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize tokenizer and lemmatizer\n",
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Data cleaning, tokenization and lemmatization\n",
    "def preprocess_tweets(tweet):\n",
    "    tweet_cleaned = re.sub(r'https?://[^ ]+', '', tweet)  # Remove URLs\n",
    "    tweet_cleaned = re.sub(r'@\\w+', '', tweet_cleaned)    # Remove mentions\n",
    "    tweet_cleaned = re.sub(r'RT', '', tweet_cleaned)      # Remove RT\n",
    "    tweet_cleaned = tweet_cleaned.lower()                 # Convert to lowercase\n",
    "    tweet_cleaned = re.sub(r'[^\\w\\s]', '', tweet_cleaned) # Remove punctuation\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = tokenizer.tokenize(tweet_cleaned)\n",
    "\n",
    "    # Stop words removal and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "g[\"tokens\"] = g[\"text\"].apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain word embeddings using gloVE library (tailored to Twitter Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the GloVe model (choose the desired model size, e.g., 'glove-twitter-100' for Twitter data)\n",
    "glove_model = api.load('glove-twitter-100')\n",
    "\n",
    "def get_tweet_embedding(tweet_tokens, glove_model):\n",
    "\n",
    "    # Retrieve GloVe embeddings for each token\n",
    "    embeddings = [glove_model[token] for token in tweet_tokens if token in glove_model]\n",
    "\n",
    "    # Handle tweets with no tokens found in the GloVe model\n",
    "    if not embeddings:\n",
    "        return None\n",
    "\n",
    "    # Aggregate the embeddings, e.g., by averaging\n",
    "    tweet_embedding = sum(embeddings) / len(embeddings)\n",
    "    return tweet_embedding\n",
    "\n",
    "g[\"tweet_embedding\"] = g[\"tokens\"].apply(lambda x : get_tweet_embedding(x, glove_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save the graph and user dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary columns \n",
    "cols_to_keep = [\"source_user_id\",\"target_user_id\",\"relationship\",\"tweet_embedding\"]\n",
    "g_filtered = g[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all null target_user_id from graph as we cannot form an edge\n",
    "g_filtered = g_filtered.drop(g_filtered[g_filtered[\"target_user_id\"].isnull()].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "user = user.drop([\"id_x\",\"id_y\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation (max-min normalisation)\n",
    "cols = ['reputation', 'retweet_ratio',\n",
    "       'url_count_tweets', 'url_ratio', 'url_max_tweets',\n",
    "       'time_interval_sd_day', 'mention_count_tweets', 'mention_max_tweets',\n",
    "       'hashtag_count_tweets', 'hashtags_max_tweets', 'age_of_account']\n",
    "\n",
    "for col in cols:\n",
    "    user[col] = (user[col] - user[col].min()) / (user[col].max() - user[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MISC] Checks to ensure that all `source_user_id` and `target_user_id` in graph dataset are included in user dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_filtered[\"source_user_id\"].isin(user[\"source_user_id\"]).sum() == len(g_filtered[\"source_user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_filtered[\"source_user_id\"].isin(user[\"source_user_id\"]).sum() == len(g_filtered[\"source_user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MISC] Checks for distribution of humans and bots in source and target user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique source user ids: 7997\n",
      "-----------------------------------------\n",
      "Number of bots: 6864\n",
      "Number of humans: 1133\n",
      "\n",
      "\n",
      "Number of unique target user ids: 15175\n",
      "-----------------------------------------\n",
      "Number of bots: 5319\n",
      "Number of humans: 9856\n"
     ]
    }
   ],
   "source": [
    "source_ids = set(g_filtered[\"source_user_id\"])\n",
    "print(f\"Number of unique source user ids: {len(source_ids)}\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "source_bot_id = 0\n",
    "source_human_id = 0\n",
    "\n",
    "for id in source_ids:\n",
    "    try:\n",
    "        if (labels[labels[\"id\"]==id][\"label\"].values[0] == 0):\n",
    "            source_human_id += 1\n",
    "        elif (labels[labels[\"id\"]==id][\"label\"].values[0] == 1):\n",
    "            source_bot_id += 1\n",
    "            \n",
    "    # if id doesnt exist in label dataset\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print(f\"Number of bots: {source_bot_id}\") \n",
    "print(f\"Number of humans: {source_human_id}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "target_ids = set(g_filtered[\"target_user_id\"])\n",
    "print(f\"Number of unique target user ids: {len(target_ids)}\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "target_bot_id = 0\n",
    "target_human_id = 0\n",
    "\n",
    "for id in target_ids:\n",
    "    try:\n",
    "        if (labels[labels[\"id\"]==id][\"label\"].values[0] == 0):\n",
    "            target_human_id += 1\n",
    "        elif (labels[labels[\"id\"]==id][\"label\"].values[0] == 1):\n",
    "            target_bot_id += 1\n",
    "            \n",
    "    # if id doesnt exist in label dataset\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print(f\"Number of bots: {target_bot_id}\") \n",
    "print(f\"Number of humans: {target_human_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of human and bots captured in this graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    10763\n",
       "0    10694\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final user dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21457 entries, 0 to 21456\n",
      "Data columns (total 34 columns):\n",
      " #   Column                Non-Null Count  Dtype              \n",
      "---  ------                --------------  -----              \n",
      " 0   verified              21457 non-null  bool               \n",
      " 1   url.urls              21457 non-null  int64              \n",
      " 2   description.urls      21457 non-null  int64              \n",
      " 3   description.mentions  21457 non-null  int64              \n",
      " 4   description.hashtags  21457 non-null  int64              \n",
      " 5   description.cashtags  21457 non-null  int64              \n",
      " 6   followers_count       21457 non-null  int64              \n",
      " 7   following_count       21457 non-null  int64              \n",
      " 8   tweet_count           21457 non-null  int64              \n",
      " 9   listed_count          21457 non-null  int64              \n",
      " 10  source_user_id        21457 non-null  int64              \n",
      " 11  username_length       21457 non-null  int64              \n",
      " 12  name_length           21457 non-null  int64              \n",
      " 13  description_length    21457 non-null  int64              \n",
      " 14  numDigits_username    21457 non-null  int64              \n",
      " 15  username_entropy      21457 non-null  float64            \n",
      " 16  description_entropy   21457 non-null  float64            \n",
      " 17  names_similarity      21457 non-null  float64            \n",
      " 18  names_ratio           21457 non-null  float64            \n",
      " 19  reputation            21457 non-null  float64            \n",
      " 20  created_at_timestamp  21457 non-null  datetime64[ns, UTC]\n",
      " 21  age_of_account        21447 non-null  float64            \n",
      " 22  label                 21457 non-null  int64              \n",
      " 23  retweet_ratio         21457 non-null  float64            \n",
      " 24  url_count_tweets      21457 non-null  float64            \n",
      " 25  url_ratio             19603 non-null  float64            \n",
      " 26  url_max_tweets        21457 non-null  float64            \n",
      " 27  time_interval_sd_day  21457 non-null  float64            \n",
      " 28  mention_count_tweets  21457 non-null  float64            \n",
      " 29  mention_max_tweets    21457 non-null  float64            \n",
      " 30  hashtag_count_tweets  21457 non-null  float64            \n",
      " 31  hashtags_max_tweets   21457 non-null  float64            \n",
      " 32  avg_tweet_length      21457 non-null  float64            \n",
      " 33  url_tweet_count       21457 non-null  float64            \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(17), int64(15)\n",
      "memory usage: 5.4 MB\n"
     ]
    }
   ],
   "source": [
    "# drop problematic columns\n",
    "user = user.drop([\"description\",\"created_at\",\"location\",\"name\",\"profile_image_url\",\"url\",\"username\"], axis=1)\n",
    "user.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_num = 0 # change to which tweet.json you are using\n",
    "user.to_csv(f\"user_{tweet_num}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43830 entries, 40 to 653128\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   source_user_id   43830 non-null  int64  \n",
      " 1   target_user_id   43830 non-null  float64\n",
      " 2   relationship     43830 non-null  object \n",
      " 3   tweet_embedding  41275 non-null  object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "g_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_num = 0 # change to which tweet.json you are using\n",
    "g_filtered.to_csv(f\"graph_{tweet_num}.csv\", index=False, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
