{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tay Han\\AppData\\Local\\Temp\\ipykernel_13708\\577788817.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_0 = pd.read_csv('user_0.csv')\n",
    "user_1 = pd.read_csv('user_1.csv')\n",
    "user_2 = pd.read_csv('user_2.csv')\n",
    "user_3 = pd.read_csv('user_3.csv')\n",
    "user_4 = pd.read_csv('user_4.csv')\n",
    "user_5 = pd.read_csv('user_5.csv')\n",
    "user_6 = pd.read_csv('user_6.csv')\n",
    "user_7 = pd.read_csv('user_7.csv')\n",
    "user_8 = pd.read_csv('user_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(user_0) + len(user_1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking the CSVs together\n",
    "\n",
    "users = pd.concat([user_0,\n",
    "                user_1, \n",
    "                user_2,\n",
    "                user_3,\n",
    "                user_4, \n",
    "                user_5,\n",
    "                user_6,\n",
    "                user_7, \n",
    "                user_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate all duplicate users within each sampling procedure\n",
    "log = {col: 'mean' if users[col].dtype in ['float64', 'int64'] else 'first' for col in users.columns if col != 'source_user_id'}\n",
    "\n",
    "users_result = users.groupby('source_user_id').agg(log).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    59769\n",
       "1.0    49656\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_result[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all tweet relationships\n",
    "\n",
    "graph_0 = pd.read_csv('graph_0.csv')\n",
    "graph_1 = pd.read_csv('graph_1.csv')\n",
    "graph_2 = pd.read_csv('graph_2.csv')\n",
    "graph_3 = pd.read_csv('graph_3.csv')\n",
    "graph_4 = pd.read_csv('graph_4.csv')\n",
    "graph_5 = pd.read_csv('graph_5.csv')\n",
    "graph_6 = pd.read_csv('graph_6.csv')\n",
    "graph_7 = pd.read_csv('graph_7.csv')\n",
    "graph_8 = pd.read_csv('graph_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n",
      "source_user_id       int64\n",
      "target_user_id     float64\n",
      "relationship        object\n",
      "tweet_embedding     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(graph_0.dtypes)\n",
    "print(graph_1.dtypes)\n",
    "print(graph_2.dtypes)\n",
    "print(graph_3.dtypes)\n",
    "print(graph_4.dtypes)\n",
    "print(graph_5.dtypes)\n",
    "print(graph_6.dtypes)\n",
    "print(graph_7.dtypes)\n",
    "print(graph_8.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking the tweet CSVs together\n",
    "\n",
    "graph = pd.concat([graph_0,\n",
    "                graph_1, \n",
    "                graph_2,\n",
    "                graph_3,\n",
    "                graph_4, \n",
    "                graph_5,\n",
    "                graph_6,\n",
    "                graph_7, \n",
    "                graph_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_result.to_csv(\"users.csv\", index = False)\n",
    "graph.to_csv(\"graph.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_user_id\n",
       "22                     1\n",
       "4440795449             1\n",
       "4439992828             1\n",
       "4439752995             1\n",
       "4439364795             1\n",
       "                      ..\n",
       "266360210              1\n",
       "266352448              1\n",
       "266348259              1\n",
       "266315980              1\n",
       "1498176452084256772    1\n",
       "Name: count, Length: 109425, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_result['source_user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tay Han\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "7.325210585076204e+17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     17\u001b[0m     source_idx \u001b[38;5;241m=\u001b[39m user_to_idx[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_user_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 18\u001b[0m     target_idx \u001b[38;5;241m=\u001b[39m \u001b[43muser_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_user_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m     idx_pair \u001b[38;5;241m=\u001b[39m (source_idx, target_idx)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (idx_pair \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m edge_list) :\n",
      "\u001b[1;31mKeyError\u001b[0m: 7.325210585076204e+17"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a mapping of user IDs to node indices >> change it up to index\n",
    "user_ids = users_result[\"source_user_id\"]\n",
    "user_labels = users_result[\"label\"]\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(users_result[\"source_user_id\"])}\n",
    "\n",
    "# Create edge index and edge weight tensors\n",
    "edge_list = []\n",
    "edge_weight_dict = {}\n",
    "\n",
    "for _, row in graph.iterrows():\n",
    "    source_idx = user_to_idx[row['source_user_id']]\n",
    "    target_idx = user_to_idx[row['target_user_id']]\n",
    "    idx_pair = (source_idx, target_idx)\n",
    "    if (idx_pair not in edge_list) :\n",
    "        edge_list.append(idx_pair)\n",
    "        edge_weight_dict[idx_pair] = 1\n",
    "    else:\n",
    "        edge_weight_dict[idx_pair] += 1\n",
    "\n",
    "edge_weight_list = list(edge_weight_dict.values())\n",
    "\n",
    "#force it to be contiguous\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(edge_weight_list, dtype=torch.float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
